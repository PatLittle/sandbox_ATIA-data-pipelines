name: Scrape Witness IDs

on:
  schedule:
    - cron: '0 11 * * *'  
  
  workflow_dispatch:


jobs:
  load_filter_ids:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v2
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas requests

    - name: Load and filter witness IDs
      run: |
        python load_and_filter_witness_ids.py

    - name: Commit and push filtered IDs
      run: |
        git config --global user.name 'github-actions'
        git config --global user.email 'github-actions@github.com'
        git pull
        git add sorted_witness_ids.csv
        git diff-index --quiet HEAD || git commit -m 'Update sorted witness IDs' && git push
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  scrape_data:
    runs-on: ubuntu-latest
    needs: load_filter_ids

    steps:
    - name: Checkout repository
      uses: actions/checkout@v2
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas requests beautifulsoup4 tqdm

    - name: Run scraping script
      run: |
        python scrape_witness_ids.py

    - name: Commit and push scraped data
      run: |
        git config --global user.name 'github-actions'
        git config --global user.email 'github-actions@github.com'
        git add full_witness_meetings_output.csv
        git commit -m 'Update witness meetings output'
        git push
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
